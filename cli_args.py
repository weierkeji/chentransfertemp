import argparse
import os
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import uvloop

uvloop.install()
from hydra import compose as hydra_compose
from hydra import initialize as hydra_init
from omegaconf import MISSING, OmegaConf

from areal.utils.fs import get_user_tmp
from realhf.api.cli_args import OptimizerConfig


@dataclass
class MicroBatchSpec:
    """Specification for splitting micro-batches during training."""

    n_mbs: int = field(
        default=1,
        metadata={
            "help": "Number of micro-batches (or minimum number if max_tokens_per_mb is set). Used when max_tokens_per_mb is None or as minimum count",
        },
    )
    max_tokens_per_mb: Optional[int] = field(
        default=None,
        metadata={
            "help": "Maximum tokens per micro-batch. When set, n_mbs becomes the minimum number of micro-batches",
        },
    )

    @classmethod
    def new(cls, mb_spec: "MicroBatchSpec", **kwargs):
        """Create new spec with updated fields while maintaining Omegaconf compatibility."""
        fields = dict(
            n_mbs=mb_spec.n_mbs,
            max_tokens_per_mb=mb_spec.max_tokens_per_mb,
        )
        fields.update(kwargs)
        return cls(**fields)


@dataclass
class GenerationHyperparameters:
    """Controls text generation behavior for RL training."""

    n_samples: int = field(
        default=8, metadata={"help": "Number of sequences to generate per prompt."}
    )
    max_new_tokens: int = field(
        default=512, metadata={"help": "Maximum number of tokens to generate."}
    )
    min_new_tokens: int = field(
        default=0, metadata={"help": "Minimum number of tokens to generate."}
    )
    max_tokens: Union[int, None] = field(
        default=None,
        metadata={
            "help": "Maximum number of tokens including prompt and generated tokens."
        },
    )
    greedy: bool = field(
        default=False,
        metadata={"help": "Whether to use greedy decoding (max probability)."},
    )
    top_p: float = field(
        default=1.0,
        metadata={"help": "Nucleus sampling probability threshold (0.0, 1.0]."},
    )
    top_k: int = field(
        default=int(1e8),
        metadata={"help": "Number of highest probability tokens to consider."},
    )
    temperature: float = field(
        default=1.0,
        metadata={"help": "Sampling temperature. Higher values increase diversity."},
    )
    stop_token_ids: List[int] = field(
        default_factory=list,
        metadata={"help": "Stop generation when encoutering these token ids."},
    )

    def new(self, **kwargs):
        args = asdict(self)
        args.update(kwargs)
        return GenerationHyperparameters(**args)


@dataclass
class PartialRolloutConfig:
    """Configuration for partial rollout."""

    mini_samples_per_group: int = field(
        default=8, metadata={"help": "Number of mini samples to train in one group"}
    )
    batch_size_exceeding_num: int = field(
        default=0, metadata={"help": "Batch size exceeding number"}
    )
    staleness_version: int = field(
        default=1, metadata={"help": "Max staleness version for the rollout."}
    )

    def new(self, **kwargs):
        args = asdict(self)
        args.update(kwargs)
        return PartialRolloutConfig(**args)


# Train Engine Configs
@dataclass
class FSDPWrapPolicy:
    transformer_layer_cls_to_wrap: Optional[List[str]] = field(
        default=None,
        metadata={"help": "A list of transformer layer names for FSDP to wrap."},
    )


@dataclass
class FSDPEngineConfig:
    wrap_policy: Optional[FSDPWrapPolicy] = field(
        default=None,
        metadata={"help": "FSDP wrap policy, specifying model layers to wrap."},
    )
    offload_params: bool = field(
        default=False,
        metadata={"help": "Whether to offload FSDP parameters to CPU."},
    )


@dataclass
class TrainControllerConfig:
    # train_engine_config: TrainEngineConfig = field(default_factory=TrainEngineConfig)
    allocation_mode: str = field(
        default="sglang.d1t8p1+d1t8p1",
        metadata={
            "help": "GPU parallel strategy allocation mode. "
            "Options: manual/heuristic or pattern-based."
        },
    )
    experiment_name: str = MISSING
    trial_name: str = MISSING
    enable_colocate_mode: bool = False
    group_size: int = 0
    storage_prefix: str = "/storage/openpsi"


@dataclass
class SchedulingSpec:
    cpu: int = field(default=0, metadata={"help": "Number of CPU cores required"})
    gpu: int = field(default=0, metadata={"help": "Number of GPU units required"})
    mem: int = field(default=0, metadata={"help": "Amount of memory (GB) required"})
    port_count: int = field(default=2, metadata={"help": "Number of ports to expose"})
    image: str = field(
        default="", metadata={"help": "Docker/Singularity container image to use"}
    )
    type: str = field(default="", metadata={"help": "Task type (e.g., worker, engine)"})
    env_vars: Dict[str, str] = field(
        default_factory=dict,
        metadata={"help": "Environment variables for the container"},
    )
    cmd: str = field(
        default="", metadata={"help": "Command to execute inside the container"}
    )


@dataclass
class SGLangConfig:
    """Configuration for SGLang runtime. Refer to:
    https://github.com/sgl-project/sglang for detailed documentation.
    """

    disable_cuda_graph: bool = False
    disable_radix_cache: bool = False
    disable_cuda_graph_padding: bool = False
    enable_nccl_nvls: bool = False
    disable_outlines_disk_cache: bool = False
    disable_custom_all_reduce: bool = False
    disable_overlap_schedule: bool = False
    enable_mixed_chunk: bool = False
    enable_dp_attention: bool = False
    enable_ep_moe: bool = False
    enable_torch_compile: bool = False
    torch_compile_max_bs: int = 32
    cuda_graph_max_bs: Optional[int] = None
    cuda_graph_bs: Optional[List[int]] = None
    torchao_config: str = ""
    enable_nan_detection: bool = False
    enable_p2p_check: bool = False
    triton_attention_reduce_in_fp32: bool = False
    triton_attention_num_kv_splits: int = 8
    num_continuous_decode_steps: int = 1
    enable_memory_saver: bool = False
    allow_auto_truncate: bool = False
    # NOTE: to avoid the illegal memory access error
    attention_backend: Optional[str] = "flashinfer"
    sampling_backend: Optional[str] = None
    context_length: Optional[int] = 32768
    mem_fraction_static: Optional[float] = 0.9
    max_running_requests: Optional[int] = None
    # NOTE: chunked_prefill_size is by default 8192 on GPUs with 80GB mem in SGLang,
    # but we disable it to avoid precision issues
    chunked_prefill_size: Optional[int] = -1
    max_prefill_tokens: int = 32768
    schedule_policy: str = "lpm"
    schedule_conservativeness: float = 1.0
    cpu_offload_gb: int = 0

    dtype: str = "float16"
    kv_cache_dtype: str = "auto"

    # logging
    log_level: str = "warning"
    log_level_http: Optional[str] = "warning"
    log_requests: bool = False
    log_requests_level: int = 0
    show_time_cost: bool = False
    enable_metrics: bool = True  # Exports Prometheus-like metrics
    # The interval (in decoding iterations) to log throughput
    # and update prometheus metrics
    decode_log_interval: int = 1

    # Use staticmethod to make OmegaConf happy.
    @staticmethod
    def build_cmd(
        sglang_config: "SGLangConfig",
        model_path,
        tp_size,
        base_gpu_id,
        dist_init_addr: Optional[str] = None,
        served_model_name: Optional[str] = None,
        skip_tokenizer_init: bool = True,
    ):
        from realhf.base import network, pkg_version, seeding
        from realhf.experiments.common.utils import asdict as conf_as_dict

        args: Dict = conf_as_dict(sglang_config)
        args["random_seed"] = seeding.get_seed()

        if served_model_name is None:
            served_model_name = model_path
        host_ip = network.gethostip()
        host = "localhost" if not sglang_config.enable_metrics else host_ip
        args = dict(
            host=host,
            model_path=model_path,
            # Model and tokenizer
            tokenizer_path=model_path,
            tokenizer_mode="auto",
            load_format="auto",
            trust_remote_code=True,
            device="cuda",
            served_model_name=served_model_name,
            is_embedding=False,
            skip_tokenizer_init=skip_tokenizer_init,
            # Other runtime options
            tp_size=tp_size,
            # Because we have set CUDA_VISIBLE_DEVICES to a single GPU in each process
            base_gpu_id=base_gpu_id,
            nnodes=1,
            node_rank=0,
            dist_init_addr=dist_init_addr,
            **args,
        )

        if pkg_version.is_version_less("sglang", "0.4.4"):
            args.pop("log_requests_level")
        if pkg_version.is_version_less("sglang", "0.4.3"):
            args.pop("enable_nccl_nvls")
            args.pop("triton_attention_num_kv_splits")
            args.pop("cuda_graph_bs")
            args.pop("enable_memory_saver")
            args.pop("allow_auto_truncate")
            args.pop("file_storage_path")

        flags = []
        for k, v in args.items():
            if v is None or v is False or v == "":
                continue
            if v is True:
                flags.append(f"--{k.replace('_', '-')} ")
                continue
            if isinstance(v, list):
                values = " ".join(map(str, v))
                flags.append(f"--{k.replace('_', '-')} {values}")
                continue
            flags.append(f"--{k.replace('_', '-')} {v}")
        flags = " ".join(flags)
        return f"python3 -m sglang.launch_server {flags}"


@dataclass
class InferenceEngineConfig:
    experiment_name: str = field(
        default=MISSING,
        metadata={"help": "Name of the experiment (no '_' or '/'). Required."},
    )
    trial_name: str = field(
        default=MISSING,
        metadata={"help": "Name of the trial (no '-' or '/'). Required."},
    )
    max_concurrent_rollouts: None | int = field(
        default=None,
        metadata={
            "help": "Maximum number of concurrent rollouts to the inference engine. Defaults to consumer_batch_size."
        },
    )
    queue_size: None | int = field(
        default=8192 * 10,
        metadata={"help": "Input/Output queue size for async rollout."},
    )
    consumer_batch_size: int = field(
        default=512,
        metadata={"help": "Batch size for consuming rollouts from the queue."},
    )
    max_head_offpolicyness: int = field(
        default=0,
        metadata={
            "help": "Maximum off-policyness for the head. "
            "If the current version is more than this many versions behind, "
            "the request will not be accepted.",
        },
    )
    # Used by remote inference engines.
    server_addrs: List[str] = field(
        default_factory=list,
        metadata={"help": "List of server addresses for inference."},
    )
    schedule_policy: str = field(
        default="round_robin",
        metadata={"help": "Request scheduling policy", "choices": ["round_robin"]},
    )
    request_timeout: float = field(
        default=7200.0, metadata={"help": "Timeout for HTTP requests."}
    )
    request_retries: int = field(
        default=3, metadata={"help": "Number of retries for failed requests."}
    )
    scheduling_specs: List[SchedulingSpec] = field(
        default_factory=list,
        metadata={"help": "inference engine schedule specs"},
    )


@dataclass
class RemoteHybridInferenceConfig(InferenceEngineConfig):
    experiment_name: str = MISSING
    trial_name: str = MISSING
    model_path: str = field(
        default=MISSING,
        metadata={"help": "model path"},
    )
    storage_path: str = field(
        default=MISSING,
        metadata={"help": "storage path"},
    )
    random_seed: int = field(
        default=0,
        metadata={"help": "random seed"},
    )
    engine_config: Dict = field(default_factory=dict)
    dp_size: int = field(
        default=1,
        metadata={"help": "dp size"},
    )
    pp_size: int = field(
        default=1,
        metadata={"help": "pp size"},
    )
    tp_size: int = field(
        default=1,
        metadata={"help": "tp size"},
    )
    seed: int = field(
        default=1,
        metadata={"help": "seed"},
    )
    batch_requests: bool = field(
        default=False,
        metadata={"help": "batch requests"},
    )
    request_timeout: float = field(
        default=7200.0, metadata={"help": "Timeout for HTTP requests."}
    )


@dataclass
class SGLangEngineConfig:
    pass


@dataclass
class RolloutControllerConfig:
    # inference_engine_config: InferenceEngineConfig = field(default_factory=InferenceEngineConfig)
    allocation_mode: str = field(
        default="sglang.d1t8p1+d1t8p1",
        metadata={
            "help": "GPU parallel strategy allocation mode. "
            "Options: manual/heuristic or pattern-based."
        },
    )
    experiment_name: str = MISSING
    trial_name: str = MISSING
    enable_colocate_mode: bool = False
    group_size: int = 0
    storage_prefix: str = "/storage/openpsi"


@dataclass
class _Timer:
    experiment_name: str = MISSING
    trial_name: str = MISSING
    fileroot: str = MISSING
    freq_epochs: Optional[int] = field(
        default=None,
        metadata={
            "help": "Trigger frequency in epochs. None disables epoch-based saving."
        },
    )
    freq_steps: Optional[int] = field(
        default=None,
        metadata={
            "help": "Trigger frequency in steps. None disables step-based saving."
        },
    )
    freq_secs: Optional[int] = field(
        default=None,
        metadata={
            "help": "Trigger frequency in seconds. None disables time-based saving."
        },
    )


@dataclass
class EvaluatorConfig(_Timer):
    pass


@dataclass
class SaverConfig(_Timer):
    pass


@dataclass
class WandBConfig:
    mode: str = "disabled"
    wandb_base_url: str = ""
    wandb_api_key: str = ""
    entity: Optional[str] = None
    project: Optional[str] = None
    name: Optional[str] = None
    job_type: Optional[str] = None
    group: Optional[str] = None
    notes: Optional[str] = None
    tags: Optional[List[str]] = None
    config: Optional[Dict] = None
    id_suffix: Optional[str] = "train"


@dataclass
class SwanlabConfig:
    project: Optional[str] = None
    name: Optional[str] = None
    config: Optional[Dict] = None
    logdir: Optional[str] = None
    mode: Optional[str] = "local"
    api_key: Optional[str] = os.getenv("SWANLAB_API_KEY", None)


@dataclass
class TensorBoardConfig:
    path: Optional[str] = None


@dataclass
class StatsLoggerConfig:
    experiment_name: str = MISSING
    trial_name: str = MISSING
    fileroot: str = MISSING
    wandb: WandBConfig = field(
        default_factory=WandBConfig,
        metadata={"help": "Weights & Biases configuration."},
    )
    swanlab: SwanlabConfig = field(
        default_factory=SwanlabConfig,
        metadata={"help": "SwanLab configuration."},
    )
    tensorboard: TensorBoardConfig = field(
        default_factory=TensorBoardConfig,
        metadata={"help": "TensorBoard configuration. Only 'path' field required."},
    )


@dataclass
class NameResolveConfig:
    type: str = field(
        default="nfs",
        metadata={
            "help": "Type of the distributed KV store for name resolving.",
            "choices": ["nfs", "etcd3", "ray"],
        },
    )
    nfs_record_root: str = field(
        default="/tmp/areal/name_resolve",
        metadata={
            "help": "Record root for NFS name resolving. Should be available in all nodes."
        },
    )
    etcd3_addr: str = field(
        default="localhost:2379", metadata={"help": "Address of the ETCD3 server."}
    )
    ray_actor_name: str = field(
        default="ray_kv_store",
        metadata={"help": "Name of the distributed Ray KV store."},
    )


@dataclass
class ClusterSpecConfig:
    name_resolve: NameResolveConfig = field(
        default_factory=NameResolveConfig,
        metadata={"help": "Name resolving configuration."},
    )
    cluster_name: str = field(
        default="local",
        metadata={"help": "Name of the cluster. Used to set specific environs."},
    )
    fileroot: str = field(
        default=get_user_tmp(),
        metadata={
            "help": "Root for logs and checkpoints. Should be available to all nodes."
        },
    )
    gpu_type: str = field(
        default="tesla", metadata={"help": "GPU type of the cluster. Used by slurm."}
    )
    mount: str = field(
        default="/storage:/storage", metadata={"help": "Mount path for slurm."}
    )
    gpu_image: str = field(default="", metadata={"help": "slurm image for trainers."})
    cpu_image: str = field(default="", metadata={"help": "slurm image for CPU jobs."})
    gpu_infer_image: str = field(
        default="", metadata={"help": "slurm image for LLM inference."}
    )
    node_name_prefix: str = field(
        default="slurmd-", metadata={"help": "Node prefix for a slurm cluster."}
    )
    n_nodes: int = field(
        default=32,
        metadata={
            "help": "The size of the cluster. Used to decide slurm hostname suffix."
        },
    )
    n_gpus_per_node: int = field(
        default=8,
        metadata={"help": "GPUs per node (physically)."},
    )


@dataclass
class TrainDataset:
    path: str = field(default="")
    max_prompt_len: int = field(default=1024)
    shuffle: bool = field(default=True)


@dataclass
class SchedulerConfig:
    endpoint: str = field(default="http://localhost:8081")
    functioncall_service_domain: str = field(default="http://localhost:8080")
    reward_functioncall_config: Dict = field(default_factory=dict)
    reward_model_path: str = field(default="")
    reward_model_service_url: str = field(default="http://localhost:30000/classify")


@dataclass
class BaseExperimentConfig:
    # NOTE: we need this unified config class because different experiments
    # have different config structures, e.g., GRPO has two engine configs,
    # but SFT only has a single one. We use subclasses to represent these structures.
    experiment_name: str = field(
        default=MISSING,
        metadata={"help": "Name of the experiment (no '_' or '/'). Required."},
    )
    trial_name: str = field(
        default=MISSING,
        metadata={"help": "Name of the trial (no '-' or '/'). Required."},
    )
    cluster: ClusterSpecConfig = field(
        default_factory=ClusterSpecConfig,
        metadata={"help": "Cluster specification. Mainly used by slurm."},
    )
    n_nodes: int = field(
        default=1, metadata={"help": "Number of nodes for experiment."}
    )
    n_gpus_per_node: int = field(
        default=8, metadata={"help": "Number of GPUs per node for this experiment."}
    )
    allocation_mode: str = field(
        default="",
        metadata={
            "help": "GPU parallel strategy allocation mode. "
            "Options: manual/heuristic or pattern-based."
        },
    )
    enable_colocate_mode: bool = field(
        default=False, metadata={"help": "Enable colocate mode."}
    )
    seed: int = field(default=1, metadata={"help": "Random seed for reproducibility."})
    total_train_epochs: int = field(
        default=1, metadata={"help": "Total number of epochs to train the model."}
    )
    total_train_steps: int = field(default=1000)
    tokenizer_path: str = field(default="")
    train_dataset: TrainDataset = field(default_factory=TrainDataset)
    stats_logger: StatsLoggerConfig = field(default_factory=StatsLoggerConfig)
    weight_update_type: str = field(default="nccl", metadata={"help": "nccl/disk"})
    scheduler: SchedulerConfig = field(default_factory=SchedulerConfig)
    train_bs_n_seqs: int = field(
        default=64, metadata={"help": "Training batch size in number of sequences"}
    )
    storage_prefix: str = field(default="/storage/openpsi")


# FSDP
@dataclass
class FSDPWrapPolicy:
    transformer_layer_cls_to_wrap: Optional[List[str]] = field(
        default=None,
        metadata={"help": "A list of transformer layer names for FSDP to wrap."},
    )


@dataclass
class FSDPEngineConfig:
    wrap_policy: Optional[FSDPWrapPolicy] = field(
        default=None,
        metadata={"help": "FSDP wrap policy, specifying model layers to wrap."},
    )
    offload_params: bool = field(
        default=False,
        metadata={"help": "Whether to offload FSDP parameters to CPU."},
    )


# remote megatron
@dataclass
class RemoteMegatronWrapPolicy:
    n_minibatches: int = 1
    kl_ctl: float = 0.0
    adv_norm: bool = False
    discount: float = 1.0
    gae_lambda: float = 1.0
    eps_clip: float = 0.2
    clip_ratio_low: float = 0.2
    clip_ratio_high: float = 0.28
    c_clip: Optional[float] = None
    value_eps_clip: float = 0.2
    max_reward_clip: float = 5.0
    disable_value: bool = True
    early_stop_kl: Optional[float] = None
    early_stop_imp_ratio: Optional[float] = None
    adaptive_kl_ctl: bool = False
    adaptive_kl_target: Optional[float] = 6
    adaptive_kl_horizon: Optional[float] = 10000
    enable_save: bool = True
    value_norm: bool = True
    value_norm_type: str = field(metadata={"choices": ["exp", "ma"]}, default="exp")
    value_norm_beta: float = 0.99995
    value_norm_eps: float = 1e-5
    group_size: int = 8
    generation_size: Optional[int] = None
    mask_no_eos_with_zero: bool = False
    group_adv_norm: bool = True
    mask_too_long: bool = False
    use_dense_reward: bool = False
    reward_delta: bool = True
    token_normalize_scope: str = field(
        default="global", metadata={"choices": ["global", "dp"]}
    )
    sample_reuse: int = 1
    temperature: float = 1.0  # GenerationHyperparameters
    reward_output_scaling: float = field(
        default=1.0, metadata={"help": "Reward scaling factor"}
    )
    reward_output_bias: float = field(default=0.0, metadata={"help": "Reward bias"})
    recompute_logp: bool = False


@dataclass
class RemoteMegatronEngineConfig:
    wrap_policy: Optional[RemoteMegatronWrapPolicy] = field(
        default_factory=RemoteMegatronWrapPolicy,
        metadata={"help": "RemoteMegatron wrap policy."},
    )
    remote_megatron_config: Dict = field(default_factory=dict)
    loss_configs: Dict = field(default_factory=dict)
    recover_dir: str = field(default="")

    @staticmethod
    def assign_wrap_policy(policy_dict: Dict) -> RemoteMegatronWrapPolicy:
        """Assign values from dictionary to RemoteMegatronWrapPolicy fields.

        Args:
            policy_dict: Dictionary containing wrap policy configuration

        Returns:
            Configured RemoteMegatronWrapPolicy instance
        """
        policy = RemoteMegatronWrapPolicy()
        for field_name, field_value in policy_dict.items():
            if hasattr(policy, field_name):
                setattr(policy, field_name, field_value)
        return policy

    experiment_name: str = field(
        default="test-exp",
        metadata={"help": "Name of the experiment (no '_' or '/'). Required."},
    )
    trial_name: str = field(
        default="test-trial",
        metadata={"help": "Name of the trial (no '-' or '/'). Required."},
    )
    group_size: int = field(
        default=8,
        metadata={"help": "Number of answers retained per prompt (best-of-n)."},
    )
    train_bs_n_seqs: int = field(
        default=32, metadata={"help": "Training batch size in number of sequences"}
    )
    n_mbs: int = field(
        default=1,
        metadata={
            "help": "Number of micro-batches (or minimum number if max_tokens_per_mb is set). Used when max_tokens_per_mb is None or as minimum count",
        },
    )
    max_tokens_per_mb: int = field(
        default=int(16384),
        metadata={
            "help": "Maximum tokens per micro-batch. When set, n_mbs becomes the minimum number of micro-batches",
        },
    )
    global_step: int = field(
        default=0,
        metadata={
            "help": "global step for recover",
        },
    )


@dataclass
class TrainEngineConfig:
    experiment_name: str = field(default="default-experiment")
    trial_name: str = field(default="trial0")
    path: str = field(default="", metadata={"help": "Path to HuggingFace checkpoint"})
    attn_impl: str = field(
        default="flash_attention_2",
        metadata={
            "help": "Attention implementation for huggingface transformers model.",
            "choices": ["flash_attention_2"],
        },
    )
    init_from_scratch: bool = field(
        default=False, metadata={"help": "Initialize model weights randomly"}
    )
    init_critic_from_actor: bool = field(
        default=False,
        metadata={"help": "Initialize critic/reward model from LM checkpoint"},
    )
    # Runtime microbatch limit
    mb_spec: MicroBatchSpec = field(default_factory=MicroBatchSpec)

    # Training Backend Configuration
    pad_mbs_to_max_tokens: bool = field(
        default=True,
        metadata={
            "help": "Pad micro-batches to configured max tokens per micro-batch"
            "when running train_batch/forward/eval_batch."
        },
    )
    gradient_checkpointing: bool = field(
        default=True, metadata={"help": "Enable gradient checkpointing"}
    )
    bf16: bool = field(default=False, metadata={"help": "Use bf16 precision"})
    optimizer: Optional[OptimizerConfig] = field(
        default=None, metadata={"help": "Optimizer configuration"}
    )
    backend: str = ""
    fsdp: FSDPEngineConfig = field(default_factory=FSDPEngineConfig)
    hybrid_engine: RemoteMegatronEngineConfig = field(
        default_factory=RemoteMegatronEngineConfig
    )
    scheduling_specs: List[SchedulingSpec] = field(
        default_factory=list,
        metadata={"help": "inference engine scheduling specs"},
    )


@dataclass
class SFTConfig(BaseExperimentConfig):
    model: TrainEngineConfig = field(default_factory=TrainEngineConfig)


@dataclass
class RecoverConfig:
    experiment_name: str = field(default="default-experiment")
    trial_name: str = field(default="trial0")
    fileroot: str = field(default="")
    recover_meta_info_path: str = field(default="")
    enable_recover: bool = field(default=False)
    latest_disable_save_hf: bool = field(
        default=True, metadata={"help": "Disable saving latest huggingFace"}
    )
    periodic_disable_save_hf: bool = field(
        default=False, metadata={"help": "Disable saving periodic huggingFace"}
    )
    periodic_save_interval: Optional[int] = field(
        default=None, metadata={"help": "Periodic save steps"}
    )
    latest_save_interval: Optional[int] = field(
        default=None, metadata={"help": "Latest save steps"}
    )
    enable_response_checkpoint: bool = field(
        default=False, metadata={"help": "Enable response checkpoint for rollout fault tolerance"}
    )


@dataclass
class GRPOConfig(BaseExperimentConfig):
    gconfig: GenerationHyperparameters = field(
        default_factory=GenerationHyperparameters
    )
    rollout: RemoteHybridInferenceConfig = field(
        default_factory=RemoteHybridInferenceConfig
    )
    actor: TrainEngineConfig = field(default_factory=TrainEngineConfig)
    ref: TrainEngineConfig = field(default_factory=TrainEngineConfig)
    recover: RecoverConfig = field(default_factory=RecoverConfig)
    partial_rollout: PartialRolloutConfig = field(default_factory=PartialRolloutConfig)


def load_expr_config(argv: List[str], config_cls) -> Tuple[BaseExperimentConfig, str]:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config", help="The path of the main configuration file", required=True
    )
    args, overrides = parser.parse_known_args(argv)

    # Initialize hydra config
    config_file = Path(args.config).absolute()
    assert config_file.exists()
    # hydra only recognize relative paths
    relpath = Path(
        os.path.relpath(str(config_file), (Path(__file__).parent).absolute())
    )
    hydra_init(config_path=str(relpath.parent), job_name="app", version_base=None)
    cfg = hydra_compose(
        config_name=str(relpath.name).removesuffix(".yaml"),
        overrides=overrides,
    )

    # Merge with the default configuration.
    # The yaml and commandline can omit some default values defined in python dataclasses.
    default_cfg = OmegaConf.structured(config_cls)
    cfg = OmegaConf.merge(default_cfg, cfg)
    cfg = OmegaConf.to_object(cfg)
    assert isinstance(cfg, BaseExperimentConfig)

    # Setup environment
    from realhf.base import constants, name_resolve

    constants.set_experiment_trial_names(cfg.experiment_name, cfg.trial_name)
    name_resolve.reconfigure(cfg.cluster.name_resolve)
    return cfg, str(config_file)
